{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2df592a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-05T04:29:46.855373Z",
     "iopub.status.busy": "2024-11-05T04:29:46.855070Z",
     "iopub.status.idle": "2024-11-05T04:30:00.092639Z",
     "shell.execute_reply": "2024-11-05T04:30:00.091483Z"
    },
    "papermill": {
     "duration": 13.244669,
     "end_time": "2024-11-05T04:30:00.095058",
     "exception": false,
     "start_time": "2024-11-05T04:29:46.850389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install fastapi uvicorn nest_asyncio pyngrok transformers torch sacremoses --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab9e5b07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T04:30:00.103541Z",
     "iopub.status.busy": "2024-11-05T04:30:00.103206Z",
     "iopub.status.idle": "2024-11-05T04:30:20.084509Z",
     "shell.execute_reply": "2024-11-05T04:30:20.083535Z"
    },
    "papermill": {
     "duration": 19.98816,
     "end_time": "2024-11-05T04:30:20.086968",
     "exception": false,
     "start_time": "2024-11-05T04:30:00.098808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, T5ForConditionalGeneration\n",
    "from fastapi import FastAPI, Form\n",
    "from pydantic import BaseModel\n",
    "from fastapi.responses import HTMLResponse\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from pyngrok import ngrok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1dbec4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T04:30:20.095327Z",
     "iopub.status.busy": "2024-11-05T04:30:20.094764Z",
     "iopub.status.idle": "2024-11-05T04:30:39.438328Z",
     "shell.execute_reply": "2024-11-05T04:30:39.437236Z"
    },
    "papermill": {
     "duration": 19.350659,
     "end_time": "2024-11-05T04:30:39.441141",
     "exception": false,
     "start_time": "2024-11-05T04:30:20.090482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42722a4bddc44c1ba8ee0be849e2b0a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/595 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1815cffbde6c4f0d90fa5e222ff08057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/927k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb95352498734c6ba68ff6d97ae87548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/696k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc61da0f1dd4f90afc3b86d34d1f5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f6bb56d4754caeb53f5edeb1984836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d522bf90391f43f1812b6b484e3591c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/1.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9cd5212f9b440c9db847e48f7bac5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.49M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e527599b8a49f083723a58bb6565a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d5665941d264a4d888e2c4722b7229d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/721 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4c25a5f3a84c0e8ff450b11e1f0ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.10G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Khởi tạo BioGPT và mô hình dịch\n",
    "bio_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BioGPT\")\n",
    "bio_model = AutoModelForCausalLM.from_pretrained(\"microsoft/BioGPT\")\n",
    "translation_tokenizer = AutoTokenizer.from_pretrained(\"VietAI/envit5-translation\")\n",
    "translation_model = T5ForConditionalGeneration.from_pretrained(\"VietAI/envit5-translation\")\n",
    "\n",
    "# Kiểm tra thiết bị và chuyển mô hình sang GPU nếu có\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "bio_model = bio_model.to(device)\n",
    "translation_model = translation_model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04fd7c7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T04:30:39.453200Z",
     "iopub.status.busy": "2024-11-05T04:30:39.452875Z",
     "iopub.status.idle": "2024-11-05T04:30:39.457434Z",
     "shell.execute_reply": "2024-11-05T04:30:39.456626Z"
    },
    "papermill": {
     "duration": 0.012637,
     "end_time": "2024-11-05T04:30:39.459283",
     "exception": false,
     "start_time": "2024-11-05T04:30:39.446646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Khởi tạo ứng dụng FastAPI\n",
    "app = FastAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9986e237",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T04:30:39.471501Z",
     "iopub.status.busy": "2024-11-05T04:30:39.471208Z",
     "iopub.status.idle": "2024-11-05T04:30:39.476583Z",
     "shell.execute_reply": "2024-11-05T04:30:39.475744Z"
    },
    "papermill": {
     "duration": 0.014066,
     "end_time": "2024-11-05T04:30:39.478447",
     "exception": false,
     "start_time": "2024-11-05T04:30:39.464381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hàm dịch\n",
    "def translate(text, direction=\"vi-en\"):\n",
    "    prompt = \"translate Vietnamese to English: \" if direction == \"vi-en\" else \"translate English to Vietnamese: \"\n",
    "    inputs = translation_tokenizer(prompt + text, return_tensors=\"pt\").to(device)\n",
    "    outputs = translation_model.generate(\n",
    "        **inputs,\n",
    "        max_length=2048,\n",
    "        num_beams=5,  # Tăng lên từ 5-10 để cải thiện chất lượng\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    return translation_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "946bdcdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T04:30:39.489352Z",
     "iopub.status.busy": "2024-11-05T04:30:39.489065Z",
     "iopub.status.idle": "2024-11-05T04:30:42.845559Z",
     "shell.execute_reply": "2024-11-05T04:30:42.844485Z"
    },
    "papermill": {
     "duration": 3.364855,
     "end_time": "2024-11-05T04:30:42.848234",
     "exception": false,
     "start_time": "2024-11-05T04:30:39.483379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok ...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 1%\r",
      "Downloading ngrok: 2%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 3%\r",
      "Downloading ngrok: 4%\r",
      "Downloading ngrok: 5%\r",
      "Downloading ngrok: 6%\r",
      "Downloading ngrok: 7%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 8%\r",
      "Downloading ngrok: 9%\r",
      "Downloading ngrok: 10%\r",
      "Downloading ngrok: 11%\r",
      "Downloading ngrok: 12%\r",
      "Downloading ngrok: 13%\r",
      "Downloading ngrok: 14%\r",
      "Downloading ngrok: 15%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 16%\r",
      "Downloading ngrok: 17%\r",
      "Downloading ngrok: 18%\r",
      "Downloading ngrok: 19%\r",
      "Downloading ngrok: 20%\r",
      "Downloading ngrok: 21%\r",
      "Downloading ngrok: 22%\r",
      "Downloading ngrok: 23%\r",
      "Downloading ngrok: 24%\r",
      "Downloading ngrok: 25%\r",
      "Downloading ngrok: 26%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 27%\r",
      "Downloading ngrok: 28%\r",
      "Downloading ngrok: 29%\r",
      "Downloading ngrok: 30%\r",
      "Downloading ngrok: 31%\r",
      "Downloading ngrok: 32%\r",
      "Downloading ngrok: 33%\r",
      "Downloading ngrok: 34%\r",
      "Downloading ngrok: 35%\r",
      "Downloading ngrok: 36%\r",
      "Downloading ngrok: 37%\r",
      "Downloading ngrok: 38%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 39%\r",
      "Downloading ngrok: 40%\r",
      "Downloading ngrok: 41%\r",
      "Downloading ngrok: 42%\r",
      "Downloading ngrok: 43%\r",
      "Downloading ngrok: 44%\r",
      "Downloading ngrok: 45%\r",
      "Downloading ngrok: 46%\r",
      "Downloading ngrok: 47%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 48%\r",
      "Downloading ngrok: 49%\r",
      "Downloading ngrok: 50%\r",
      "Downloading ngrok: 51%\r",
      "Downloading ngrok: 52%\r",
      "Downloading ngrok: 53%\r",
      "Downloading ngrok: 54%\r",
      "Downloading ngrok: 55%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 56%\r",
      "Downloading ngrok: 57%\r",
      "Downloading ngrok: 58%\r",
      "Downloading ngrok: 59%\r",
      "Downloading ngrok: 60%\r",
      "Downloading ngrok: 61%\r",
      "Downloading ngrok: 62%\r",
      "Downloading ngrok: 63%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 64%\r",
      "Downloading ngrok: 65%\r",
      "Downloading ngrok: 66%\r",
      "Downloading ngrok: 67%\r",
      "Downloading ngrok: 68%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 69%\r",
      "Downloading ngrok: 70%\r",
      "Downloading ngrok: 71%\r",
      "Downloading ngrok: 72%\r",
      "Downloading ngrok: 73%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 74%\r",
      "Downloading ngrok: 75%\r",
      "Downloading ngrok: 76%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 77%\r",
      "Downloading ngrok: 78%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 79%\r",
      "Downloading ngrok: 80%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 81%\r",
      "Downloading ngrok: 82%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 83%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 84%\r",
      "Downloading ngrok: 85%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 86%\r",
      "Downloading ngrok: 87%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 88%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 89%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 90%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 91%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 92%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 93%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 94%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 95%\r",
      "Downloading ngrok: 96%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 97%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 98%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 99%\r",
      "Downloading ngrok: 100%\r",
      "                                                                                                    \r",
      "Installing ngrok ... \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r",
      "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\r\n"
     ]
    }
   ],
   "source": [
    "!ngrok authtoken 2oPcvV2YiJScnYzCORQaPbuHP5q_4Q8A8E6j6V8vEMQpMJrVr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e72bfac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T04:30:42.866466Z",
     "iopub.status.busy": "2024-11-05T04:30:42.866107Z",
     "iopub.status.idle": "2024-11-05T04:30:42.873198Z",
     "shell.execute_reply": "2024-11-05T04:30:42.872129Z"
    },
    "papermill": {
     "duration": 0.018561,
     "end_time": "2024-11-05T04:30:42.875294",
     "exception": false,
     "start_time": "2024-11-05T04:30:42.856733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Trang chủ với form nhập liệu đơn giản\n",
    "@app.get(\"/\", response_class=HTMLResponse)\n",
    "async def home():\n",
    "    return \"\"\"\n",
    "    <html>\n",
    "        <head>\n",
    "            <title>BioGPT API</title>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>BioGPT API</h1>\n",
    "            <form action=\"/process_biogpt\" method=\"post\">\n",
    "                <label for=\"sentence\">Nhập câu cần xử lý:</label><br>\n",
    "                <input type=\"text\" id=\"sentence\" name=\"sentence\" style=\"width: 300px;\"><br><br>\n",
    "                <label for=\"translate_to_vietnamese\">Sử dụng tiếng Việt:</label>\n",
    "                <input type=\"checkbox\" id=\"translate_to_vietnamese\" name=\"translate_to_vietnamese\"><br><br>\n",
    "                <input type=\"submit\" value=\"Gửi yêu cầu\">\n",
    "            </form>\n",
    "        </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6271122e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T04:30:42.894086Z",
     "iopub.status.busy": "2024-11-05T04:30:42.893789Z",
     "iopub.status.idle": "2024-11-05T04:30:42.907682Z",
     "shell.execute_reply": "2024-11-05T04:30:42.907019Z"
    },
    "papermill": {
     "duration": 0.025396,
     "end_time": "2024-11-05T04:30:42.909557",
     "exception": false,
     "start_time": "2024-11-05T04:30:42.884161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Endpoint để xử lý câu và trả về kết quả từ BioGPT\n",
    "@app.post(\"/process_biogpt\", response_class=HTMLResponse)\n",
    "async def process_biogpt(\n",
    "    sentence: str = Form(...),\n",
    "    translate_to_vietnamese: bool = Form(False)\n",
    "):\n",
    "    # Chuẩn bị đầu vào cho BioGPT\n",
    "    if translate_to_vietnamese:\n",
    "        sentence = translate(sentence, \"en-vi\")\n",
    "    inputs = bio_tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        beam_output = bio_model.generate(\n",
    "            **inputs, min_length=100, max_length=1024, num_beams=5,\n",
    "            early_stopping=True, temperature=0.3, top_p=0.8, repetition_penalty=2.0,\n",
    "            no_repeat_ngram_size=5, do_sample=True\n",
    "        )\n",
    "    output_text = bio_tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Dịch lại nếu có chọn tùy chọn\n",
    "    if translate_to_vietnamese:\n",
    "        output_text = translate(output_text, \"en-vi\")\n",
    "\n",
    "    # Trả về trang HTML với kết quả\n",
    "    return f\"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <h2>Kết quả từ BioGPT:</h2>\n",
    "            <p>{output_text}</p>\n",
    "            <a href=\"/\">Quay lại</a>\n",
    "        </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e34feebf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T04:30:42.925322Z",
     "iopub.status.busy": "2024-11-05T04:30:42.925053Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2024-11-05T04:30:42.916956",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Waiting for application startup.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Application startup complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public URL: NgrokTunnel: \"https://cdcd-34-44-37-82.ngrok-free.app\" -> \"http://localhost:8000\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"GET / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"POST /process_biogpt HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"GET / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"POST /process_biogpt HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"GET / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"POST /process_biogpt HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"GET / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"POST /process_biogpt HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"GET / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"POST /process_biogpt HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"GET / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"POST /process_biogpt HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"GET / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"POST /process_biogpt HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"GET / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"POST /process_biogpt HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"GET / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"POST /process_biogpt HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"GET / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"POST /process_biogpt HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"GET / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"POST /process_biogpt HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"GET / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"POST /process_biogpt HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"GET / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"POST /process_biogpt HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"GET / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"POST /process_biogpt HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     42.118.45.46:0 - \"GET / HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "# Chạy server với ngrok\n",
    "if __name__ == \"__main__\":\n",
    "    # Cho phép async trong notebook\n",
    "    nest_asyncio.apply()\n",
    "    \n",
    "    # Khởi tạo ngrok tunnel\n",
    "    public_url = ngrok.connect(8000)\n",
    "    print(\"Public URL:\", public_url)\n",
    "\n",
    "    # Chạy server\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-05T04:29:44.171934",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}